{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b73858b-b9f1-4ed0-971a-80a7950b1e7b",
   "metadata": {},
   "source": [
    "**Note:** a spider doesn't see the webpage as we do. a spider sees it without javascript. So to scrape data from javascript fields and interact with the webpage we use something like splash or, slenium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc3fbf-1a53-406e-aadb-d36315842ec8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scrapy Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d3b5f-ef22-4f34-9522-5a670c20fb29",
   "metadata": {},
   "source": [
    "Scrapy has 5 main components.\n",
    "- **spiders:** this is the file where we define what we want to scrape from a webpage. scrapy has 5 built-in spider classes (scrapy.Spider, CrawlSpider, XMLFeedSpider, CSVFeedSpider, SitemapSpider).\n",
    "- **pipelines:** this is where the extracted data is manipulated (cleaning of data, removal of duplication, populating data in databases or other external storage files).\n",
    "- **middlewares:** has everythig to do with the request sent and response received. if we want to inject some custom headers or proxies then we have do it through middlewares.\n",
    "- **engine:** it ensures the consistency of the whole operation, meaning, it coordinates the operation of other components.\n",
    "- **scheduler:** responsible for preserving the order of operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b95ae-3d77-439a-b0e5-e83bbb28994b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scrapy Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713af29b-06de-4f35-9fe2-acd8fcfda88a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Basic Scrapy commands "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec3b2353-66c0-40e8-9323-5087b02b44bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapy 2.5.0 - no active project\n",
      "\n",
      "Usage:\n",
      "  scrapy <command> [options] [args]\n",
      "\n",
      "Available commands:\n",
      "  bench         Run quick benchmark test\n",
      "  commands      \n",
      "  fetch         Fetch a URL using the Scrapy downloader\n",
      "  genspider     Generate new spider using pre-defined templates\n",
      "  runspider     Run a self-contained spider (without creating a project)\n",
      "  settings      Get settings values\n",
      "  shell         Interactive scraping console\n",
      "  startproject  Create new project\n",
      "  version       Print Scrapy version\n",
      "  view          Open URL in browser, as seen by Scrapy\n",
      "\n",
      "  [ more ]      More commands available when run from project directory\n",
      "\n",
      "Use \"scrapy <command> -h\" to see more info about a command\n"
     ]
    }
   ],
   "source": [
    "# !scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f34aa62-61a1-459d-8fca-3b1b42d6d11a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating a Scrapy Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7d0418",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f57b227-e9de-4a1b-a7be-f46f3bb8a41d",
   "metadata": {},
   "source": [
    "**scrapy startproject project_name**. This will create a folder named \"project_name\" in the current working directory and start a project using scrapy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d3ef92-4ce0-4b10-b02a-a57480afa129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'worldometer', using template directory '/home/maidul-hasan/miniconda3/envs/scraper_env/lib/python3.8/site-packages/scrapy/templates/project', created in:\n",
      "    /home/maidul-hasan/Work/Web Scraping/worldometer\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd worldometer\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "# ! scrapy startproject worldometer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c818aa-7cb7-49d9-86f3-7234aaee1d2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Listing the files and folders inside the project directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "702a0208-c5ba-4025-b799-a925a833bb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirpath:  ./worldometer/\n",
      "directories:  ['worldometer']\n",
      "files:  ['scrapy.cfg'] \n",
      "\n",
      "dirpath:  ./worldometer/worldometer\n",
      "directories:  ['spiders']\n",
      "files:  ['__init__.py', 'settings.py', 'items.py', 'pipelines.py', 'middlewares.py'] \n",
      "\n",
      "dirpath:  ./worldometer/worldometer/spiders\n",
      "directories:  []\n",
      "files:  ['__init__.py'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for dirpath, directories, files in os.walk(\"./worldometer/\"):\n",
    "    print(\"dirpath: \", dirpath)\n",
    "    print(\"directories: \", directories)\n",
    "    print(\"files: \", files, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e0b194-18ca-49a0-b270-9f31de6320c0",
   "metadata": {},
   "source": [
    "- ./worldometer/worldometer/spiders/: this is where all the spiders live.\n",
    "- ./worldometer/scrapy.cfg: this file is very important to execute the spiders we create. it is also used to deploy our spiders to heroku or other hosting services.\n",
    "- ./worldometer/worldometer/items.py: we use this to clean the data we scrape and store the scraped data in different fields that we create.\n",
    "- ./worldometer/worldometer/middlewares.py: it does everything that has something to do with the request we send and the response that we get. we can write our own middlware class to manipulate actions associated with request and response.\n",
    "- ./worldometer/worldometer/pipelines.py: used in order to channel the items we have scraped to a database.\n",
    "- ./worldometer/worldometer/settings.py: used for extra tweaking and configuration of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe5a40-c5d2-4458-928d-185dd20c8f61",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generating a Spider "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05b48deb-cf65-4a88-ac32-42f8a1861439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd project_folder\n",
    "# scrapy genspider -t template_name spider_name allowed_domain_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa1ce0-c0b9-4ebb-8e17-10ac586790de",
   "metadata": {},
   "source": [
    "The default template name is basic and it just provides a .py file that contains a barebone spider class (which inherits from scrapy.Spider). We can have multiple spiders but their names must be unique. The url we supply to scrapy has to be without the front 'https://' and the end '/' as scrapy will handle that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365dc0cf-1545-46bb-9b49-86ba615ca3e7",
   "metadata": {},
   "source": [
    "Remember that you can change the predefined names (variable, attribute, class or method names) in the main spider (that is, the .py file created when you generated the spider) by tweaking the settings but you should not change them on their own as that would break the spider. So it's best if you leave them alone as is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7f3aee-da59-44b6-9130-479977ead2d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Before you start scraping Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3a724-bee4-4d0d-9ef3-0c90b4ee10f5",
   "metadata": {},
   "source": [
    "Use chrome or chromium for web scraping purpose as chrome web store has some preety good add-ons that will help you in the whole process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3937d850-3fdb-493c-8a44-996604350e77",
   "metadata": {},
   "source": [
    "- First disable javascript. \n",
    "    - open the devoloper tools window (ctrl+shift+i). This lets you inspect an element.\n",
    "    - go to the command pallete (ctrl + shift + p).\n",
    "    - type in javascript and press Disable JavaScript. you can re-enable javascript following the same steps.\n",
    "- Select the inspection tool and then select the element to be inspected (extracted) to see the html tags of that element.\n",
    "- To see if we can extract the desired data open devoloper tools window (ctrl+shift+i) and press (ctrl+f) to try out a 'css' or, 'xpath' selector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f2be67-150b-4b07-a25b-3753457596ad",
   "metadata": {},
   "source": [
    "It is necessery to disable javascript in order to make sure that we are seeing the same html webpage as the spider itself. This way writing a css or, xpath selector becomes easy and we can know if we can extract an element using scrapy only or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa97994-085e-466d-ae1f-82dd6722109a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CSS Selectors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92011df2-ef7a-4d20-9f24-603abf4722e9",
   "metadata": {},
   "source": [
    "In html web pages all elements are tagged with some html tags. For example, a paragraph is tagged with the \\<p> tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b5a7e9-5b4d-4ad2-8ea6-ad6a79d3783a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Selecting elements by their tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7f90d6-38b2-4213-b92e-0281c4918290",
   "metadata": {},
   "source": [
    "1. To select all the elements with a particular <u>tag name</u>, **syntax: tagName**. It's as simple as that. But if we want to select one particular element or some certain elements then this is not the clever way to do it. Instead we should target elements either by their <u>class attribute, id or by position</u> so we can limit the scope of the CSS selector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc44083-ef25-4aff-a553-fcc62a5ebcc4",
   "metadata": {},
   "source": [
    "2. To select any element by its <u>class attribute value</u>, **syntax: .className**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092a150-5498-4976-b02b-f48941f128a6",
   "metadata": {},
   "source": [
    "3. To target an element by its <u>id attribute value</u>, **syntax: #id**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb3188-ff70-44ec-9f5c-a09f3982543a",
   "metadata": {},
   "source": [
    "Note: The same exact class attribute value can be assigned to more than one element. However, an id can only be assigned to a particular element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017de12-5538-44da-80cb-1daaef811b84",
   "metadata": {},
   "source": [
    "**Example:** Let’s say we want to select the “p” element that has a class attribute equals to “intro” and an id value \"outside\". In this case we can use one of the following CSS selectors, **p.intro** or, **#outside**, to select the element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043dff09-facf-45a1-8c03-f0f33af4d14b",
   "metadata": {},
   "source": [
    "4. To select an element that has <u>multiple classes</u> (for example, 'bold italic'), **syntax: tagName.className.2nd_className** and so on and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388a630c-b761-4205-aeb9-af0dc165dfba",
   "metadata": {},
   "source": [
    "5. To select an element that has <u>some other attribute name and value</u>, **syntax: tagName [attributeName=attributeValue]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c6995-44bd-4f2d-8dd6-f8e392d8736f",
   "metadata": {},
   "source": [
    "6. To select an element where <u>an attribute (for example the 'href' attribute)</u>, \n",
    "    - <u>starts with some certain letters</u>, **syntax: tagName[href^=\"https\"]**\n",
    "    - <u>ends with some certain letters</u>, **syntax: tagName[href$=\"com\"]**\n",
    "    - <u>contains some certain letters</u>, <b>syntax: tagName[href*=\"google\"]</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92d235-0287-441b-9b84-0bd7b43b5489",
   "metadata": {},
   "source": [
    "7. To <u>extract the link within a tag(usually 'a' tag)</u>, **syntax: a::attr(href)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b322c-de53-40b0-a15b-3507b6aacb4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CSS combinators, Selecting elements by their relative position "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8365fe6f-3878-418c-be95-4d17a7f507eb",
   "metadata": {},
   "source": [
    "1. To get all the elements that are placed inside a particular element, **syntax: outerElementTag.outerElementClass innerElementTag**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8dace7-599f-4811-9343-a98fd905a1e5",
   "metadata": {},
   "source": [
    "2. To get all the elements that are immediate children of a specific element, **syntax: .specificElementClass > childElementsTag**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce74252-b97b-4444-bbd9-0b769e713dfa",
   "metadata": {},
   "source": [
    "3. To get the n'th child of a certain element, **syntax: certainElementTag:nth-child(childsPosition)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b65a58-35a3-4465-8b64-2efb2c27af29",
   "metadata": {},
   "source": [
    "4. To select the element that is placed immediately after a particular element, **syntax: #firstElementId + #secondElementId**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c3e755-eaa5-422b-8289-51f1410897a4",
   "metadata": {},
   "source": [
    "5. To select all the elements that are placed after a particular element anywhere in the code but not necessarily immediately after it, **syntax: #firstElementId ~ .otherElementsClass**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c1e40d-ddd2-409d-9899-ed981a4f465a",
   "metadata": {},
   "source": [
    "To get multiple of these nth childs use, **certainElementTag:nth-child(childsPosition), certainElementTag:nth-child(childsPosition), ...........**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b4c48-4c12-40ee-901e-652048a379c9",
   "metadata": {},
   "source": [
    "## Modifying the settings.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d3ccc",
   "metadata": {},
   "source": [
    "    ROBOTSTXT_OBEY = False\n",
    "    FEED_EXPORT_ENCODING = 'utf-8'\n",
    "    \n",
    "    # to avoid repeated requests to the same link.\n",
    "    AUTOTHROTTLE_ENABLED = True\n",
    "    HTTPCACHE_ENABLED = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61174c98",
   "metadata": {},
   "source": [
    "## Avoid getting banned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64bb10b",
   "metadata": {},
   "source": [
    "### User-Agent Rotator Middleware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b2d94",
   "metadata": {},
   "source": [
    "Modify the middlewares.py file to choose random user agent from a list of user agents to try avoid getting banned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41691a28",
   "metadata": {},
   "source": [
    "    import logging\n",
    "    import random\n",
    "    \n",
    "    from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware\n",
    "\n",
    "\n",
    "    class UserAgentRotatorMiddleware(UserAgentMiddleware):\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 \"\n",
    "            \"Safari/537.36 Edge/12.246\",\n",
    "            \"Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 \"\n",
    "            \"Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 \"\n",
    "            \"Safari/601.3.9\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.111 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:15.0) Gecko/20100101 Firefox/15.0.1\"\n",
    "        ]\n",
    "\n",
    "        def __init__(self, user_agent=\"\"):\n",
    "            self.user_agent = user_agent\n",
    "\n",
    "        def process_request(self, request, spider):\n",
    "            try:\n",
    "                self.user_agent = random.choice(self.user_agents)\n",
    "                request.headers.setdefault(\"User-Agent\", self.user_agent)\n",
    "            except IndexError:\n",
    "                logging.error(\"Couldn't fetch the User Agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dacc9d",
   "metadata": {},
   "source": [
    "## Debugging the spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350c38de",
   "metadata": {},
   "source": [
    "### Using scrapy's default functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35c131a",
   "metadata": {},
   "source": [
    "Read the docs at: <a>https://docs.scrapy.org/en/latest/topics/debug.html</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34a116d",
   "metadata": {},
   "source": [
    "### Using the Debug option of an IDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53618815",
   "metadata": {},
   "source": [
    "First create a python file. i.e, debug.py. The debug.py file should look like this - \n",
    "    \n",
    "    import scrapy\n",
    "    from scrapy.crawler import CrawlerProcess\n",
    "    from scrapy.utils.project import get_project_settings\n",
    "    from Project_Name.spiders.Spider_Name import Spider_Class_Name\n",
    "\n",
    "    process = CrawlerProcess(settings=get_project_settings())\n",
    "    process.crawl(Spider_Class_Name)\n",
    "    process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d6a5a7",
   "metadata": {},
   "source": [
    "Next, define the break point in the spider itself and then, run the debug.py file in the Start Debugging mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af45439e",
   "metadata": {},
   "source": [
    "## Available templates to generate different types of spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed6100",
   "metadata": {},
   "source": [
    "Available templates:\n",
    "- basic : The basic template generates a basic spider that inherits from the scrapy.Spider class. Out of the box it contains the spider name, allowed domains, start_urls list or a start_requests class method that utilizes scrapy.Request to send the initial request and a parse method.   \n",
    "- crawl : The crawl template generates a crawl spider that inherits from the CrawlSpider class. The crawl spider contains a special tuple named \"rules\". In this tuple we define certain Rule(s) that the crawler obeys when extracting links. We can say it to extract links if the href contains certain word or we can define its working zone by defining a css/xpath expression.One thing to note that the callback method should never be named \"parse\".\n",
    "The CrawlSpider is very useful when you have many href links for a particular css/xpath selector but you only want to follow links that has some certain properties to it. In this case you can define a Rule object to do that without hassle.\n",
    "- csvfeed : Used to scrape csv files. \n",
    "- xmlfeed : Used to scrape xml files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ae93a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
